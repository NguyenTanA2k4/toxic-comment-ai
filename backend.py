# backend.py - PHIÃŠN Báº¢N HOÃ€N Háº¢O (IP Log + Hybrid + Fix Frontend)
from fastapi import FastAPI, Request
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import uvicorn
import re
import os

app = FastAPI(title="Toxic Comment Detection API")

# --- 1. Cáº¤U HÃŒNH & LOAD DATA ---
MODEL_PATH = "./model"
BLACKLIST_FILE = "blacklist.txt"
BLACKLIST = []

# Load Model
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)
    print("âœ… ÄÃ£ load Model thÃ nh cÃ´ng!")
except Exception as e:
    print(f"âŒ Lá»—i load model: {e}")

# Load Blacklist
if os.path.exists(BLACKLIST_FILE):
    with open(BLACKLIST_FILE, "r", encoding="utf-8") as f:
        BLACKLIST = [line.strip().lower() for line in f if line.strip()]
    print(f"âœ… ÄÃ£ load {len(BLACKLIST)} tá»« cáº¥m tá»« {BLACKLIST_FILE}")
else:
    print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y {BLACKLIST_FILE}. Chá»‰ dÃ¹ng AI.")

# --- 2. HÃ€M Xá»¬ LÃ TEXT ---
teencode_dict = {
    "tk": "tháº±ng", "mk": "mÃ¬nh", "nguu": "ngu", "nguuu": "ngu",
    "m": "mÃ y", "t": "tao", "k": "khÃ´ng", "ko": "khÃ´ng",
    "cc": "cá»¥c cá»©t", "cl": "cÃ¡i lá»“n", "loz": "lá»“n", "dm": "Ä‘á»‹t máº¹", "vcl": "vÃ£i cáº£ lá»“n"
}

def clean_text(text: str):
    text = text.lower()
    text = re.sub(r'([a-z])\1+', r'\1', text) 
    words = text.split()
    fixed_words = [teencode_dict.get(word, word) for word in words]
    return " ".join(fixed_words)

def check_blacklist(text):
    for word in BLACKLIST:
        if word in text: 
            return True, word
    return False, None

# Input Model
class TextRequest(BaseModel):
    text: str

@app.get("/")
def home():
    return {"message": "Server Ä‘ang cháº¡y ngon lÃ nh!"}

# --- 3. API Dá»° ÄOÃN (CÃ“ TRACKING IP) ---
@app.post("/predict")
async def predict(data: TextRequest, request: Request):
    original_text = data.text
    
    # === A. Báº®T Äá»ŠA CHá»ˆ IP ===
    forwarded = request.headers.get("x-forwarded-for")
    if forwarded:
        client_ip = forwarded.split(",")[0]
    else:
        client_ip = request.client.host
    
    print(f"ðŸ‘€ IP [{client_ip}] Ä‘ang check: '{original_text}'", flush=True)
    # ==========================

    # Xá»­ lÃ½ vÄƒn báº£n
    processed_text = clean_text(original_text)
    
    # BÆ¯á»šC 1: KIá»‚M TRA BLACKLIST
    is_toxic = False
    score = 0.0
    label = "CLEAN"
    
    if BLACKLIST:
        is_blacklisted, banned_word = check_blacklist(processed_text)
        if is_blacklisted:
            is_toxic = True
            score = 1.0 # Max Ä‘iá»ƒm vÃ¬ trÃºng tá»« cáº¥m
            label = "TOXIC"
            print(f"   -> â›” Bá»Š CHáº¶N Bá»žI BLACKLIST (Tá»«: {banned_word})", flush=True)
            return {"label": label, "score": score}

    # BÆ¯á»šC 2: AI Dá»° ÄOÃN
    inputs = tokenizer(processed_text, return_tensors="pt", truncation=True, padding=True, max_length=128)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
        score = probs[0][1].item() # Láº¥y Ä‘iá»ƒm Toxic

    if score > 0.5:
        label = "TOXIC"
    else:
        label = "CLEAN"

    print(f"   -> ðŸ¤– AI cháº¥m Ä‘iá»ƒm: {label} ({score:.2f})", flush=True)

    return {"label": label, "score": score}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)